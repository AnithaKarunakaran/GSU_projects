{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSA 8040 Project III - Fall 2020\n",
    "-- Aishat Olayiwola, Anitha Karunakaran, Patrick Silva, Weiting Yu\n",
    "\n",
    "### Web Scraping Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import csv\n",
    "import time \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import requests, re \n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1\n",
    "\n",
    "### This step gathers all URLs from the main page without filtering the content of the transcript (All sectors)\n",
    "There are 30 items per page with a total of 4391793 (assuming the data-id HTML element represents unique id number of that transcript) as the last data-id on Nov27 @ 10:07PM. Therefore, we'll have ~146,393.1 pages to scrape, where only unique transcripts are shown in each page. However, as of today, the last page with information is 6427 (information from Apr. 24, 2004, 9:34 AM).\n",
    "\n",
    "**Notes:**\n",
    "- a) We only want data for year 2019 and 2020 (beginning at p.1169 as of Dec 06 11:22PM).\n",
    "- b) Not all transcripts are shown as some of the pages jump the data-if by 2 or more numbers.\n",
    "- c) Some transcripts requires a subscription to be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA8040_SeekingAlphaBot:\n",
    "\n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.driver = webdriver.Firefox()\n",
    "\n",
    "    def closeBrowser(self):\n",
    "        self.browser.close()\n",
    "\n",
    "    def login(self):\n",
    "        #Visits the Website\n",
    "        driver = self.driver\n",
    "        driver.get(\"https://seekingalpha.com\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #Click on Sign In Button and Proceeds with the Login\n",
    "        login_button = driver.find_element_by_xpath(\"//button[@data-test-id='header-button-sign-in']\")\n",
    "        login_button.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        ##Login Parameters\n",
    "        user_name_elem = driver.find_element_by_xpath(\"//input[@name='email']\")\n",
    "        user_name_elem.clear()\n",
    "        user_name_elem.send_keys(self.username)\n",
    "        passworword_elem = driver.find_element_by_xpath(\"//input[@name='password']\")\n",
    "        passworword_elem.clear()\n",
    "        passworword_elem.send_keys(self.password)\n",
    "        passworword_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #Sets the URL coming from phase 1\n",
    "        i = 1\n",
    "        data_set_phase1 = []\n",
    "        \n",
    "        while i <= 1169: \n",
    "       \n",
    "            base_url=\"https://seekingalpha.com/earnings/earnings-call-transcripts/\"\n",
    "\n",
    "            time.sleep(2) \n",
    "            \n",
    "            driver.get(base_url+str(i))\n",
    "            time.sleep(2)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            all=soup.find_all(\"li\",{\"class\":\"list-group-item article\"})\n",
    "\n",
    "            for item in all:\n",
    "\n",
    "                #enter dictionary for store the data\n",
    "                d={}\n",
    "                #--------------------------------------------------#\n",
    "                d[\"Title\"]=item.find(\"h3\",{\"class\":\"list-group-item-heading\"}).text.replace(\"\\n\",\"\")\n",
    "\n",
    "                #--------------------------------------------------# \n",
    "                try:\n",
    "                    d[\"Url\"]=\"https://seekingalpha.com\"+item.find(\"a\")['href']\n",
    "\n",
    "                except:\n",
    "                    d[\"Url\"]=None\n",
    "                #--------------------------------------------------#              \n",
    "                data_set_phase1.append(d)\n",
    "\n",
    "            i += 1\n",
    "            print(i)\n",
    "            \n",
    "        #--------------------------------------------------------------------------------------------#    \n",
    "        df = pd.DataFrame(data_set_phase1)\n",
    "            \n",
    "        # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "        writer = pd.ExcelWriter('new_urls.xlsx', engine='xlsxwriter')\n",
    "            \n",
    "        # Convert the dataframe to an XlsxWriter Excel object.\n",
    "        df.to_excel(writer, sheet_name='Sheet1')\n",
    "            \n",
    "        # Close the Pandas Excel writer and output the Excel file.\n",
    "        writer.save()            \n",
    "            \n",
    "Project3 = MSA8040_SeekingAlphaBot(\"patricks.s@hotmail.com\",\"msa80402020\")\n",
    "Project3.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_set_phase1 = pd.read_excel(r'C:\\Users\\patri\\Documents\\GSU\\MSA 8040 - Data Management\\Project 03\\new_urls.xlsx')\n",
    "\n",
    "df_phase1=pd.DataFrame(df_data_set_phase1[[\"Title\",\"Url\"]]).drop_duplicates()\n",
    "\n",
    "display(df_phase1.iloc[0,1])\n",
    "display(df_phase1.shape)\n",
    "\n",
    "#Dropping Duplicates (if any)\n",
    "df_url=pd.DataFrame(df_phase1[\"Url\"]).drop_duplicates()\n",
    "df_url.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2\n",
    "\n",
    "-- In this phase, we used the Bot to scrap the data from the URLs gathered from Phase 1. Each URL is a transcript and the result of the Phase 2 is a dataframe where each row is the content of the transcript.\n",
    "\n",
    "-- Note that the process during Phase I has originated a dataframe with 17,190 unique URLs. Since running all URLs is computationally expensive, we randomly selected 2,000 URLs in order to get a more diverse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Sample\n",
    "def get_sample(df, col_name, n=100, seed=42):\n",
    "    \"\"\"Get a sample from a column of a dataframe.\n",
    "    \n",
    "    It drops any numpy.nan entries before sampling. The sampling\n",
    "    is performed without replacement.\n",
    "    \n",
    "    Example of numpydoc for those who haven't seen yet.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Source dataframe.\n",
    "    col_name : str\n",
    "        Name of the column to be sampled.\n",
    "    n : int\n",
    "        Sample size. Default is 100.\n",
    "    seed : int\n",
    "        Random seed. Default is 42.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Sample of size n from dataframe's column.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    random_idx = np.random.choice(df[col_name].dropna().index, size=n, replace=False)\n",
    "    \n",
    "    return df.loc[random_idx, col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_set_phase1 = pd.read_excel(r'C:\\Users\\patri\\Documents\\GSU\\MSA 8040 - Data Management\\Project 03\\new_urls.xlsx')\n",
    "\n",
    "df_phase1=pd.DataFrame(df_data_set_phase1[[\"Title\",\"Url\"]]).drop_duplicates()\n",
    "\n",
    "display(df_phase1.iloc[0,1])\n",
    "display(df_phase1.shape)\n",
    "\n",
    "#Dropping Duplicates\n",
    "df_url=pd.DataFrame(df_phase1[\"Url\"]).drop_duplicates()\n",
    "df_url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = pd.DataFrame(get_sample(df_url,\"Url\",2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA8040_SeekingAlphaBot:\n",
    "\n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.driver = webdriver.Firefox()\n",
    "\n",
    "    def closeBrowser(self):\n",
    "        self.browser.close()\n",
    "\n",
    "    def login(self):\n",
    "        #Visits the Website\n",
    "        driver = self.driver\n",
    "        driver.get(\"https://seekingalpha.com\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #Click on Sign In Button and Proceeds with the Login\n",
    "        login_button = driver.find_element_by_xpath(\"//button[@data-test-id='header-button-sign-in']\")\n",
    "        login_button.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        ##Login Parameters\n",
    "        user_name_elem = driver.find_element_by_xpath(\"//input[@name='email']\")\n",
    "        user_name_elem.clear()\n",
    "        user_name_elem.send_keys(self.username)\n",
    "        passworword_elem = driver.find_element_by_xpath(\"//input[@name='password']\")\n",
    "        passworword_elem.clear()\n",
    "        passworword_elem.send_keys(self.password)\n",
    "        passworword_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2) \n",
    "        \n",
    "        #============================================== PHASE 2 ===============================================#\n",
    "        i = 0\n",
    "        base_url = \"\"\n",
    "        transcript_title = []\n",
    "        transcript_content = []\n",
    "         \n",
    "        while i <= len(sample_url)-1: \n",
    "            \n",
    "            base_url = str(sample_url.iloc[i,0])\n",
    "            \n",
    "            time.sleep(2) \n",
    "            driver.get(base_url)\n",
    "            time.sleep(2)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            all1=soup.find_all(\"div\",{\"class\":\"sa-art-hd\"}) #title\n",
    "            all2=soup.find_all(\"div\",{\"class\":\"sa-art article-width\"}) #Content\n",
    "            \n",
    "            #------------------------------------ TITLE --------------------------------------------------------#\n",
    "            for item in all1:\n",
    "\n",
    "                # enter dictionary for store the data\n",
    "                d1 = {}\n",
    "                try:\n",
    "                    d1[\"Title\"] = str(item.find(\"h1\")).split(\">\")[1].split(\"<\")[0]\n",
    "                    time.sleep(2)\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                transcript_title.append(d1)\n",
    "\n",
    "            #------------------------------------ CONTENT --------------------------------------------------------#\n",
    "            for item in all2:\n",
    "\n",
    "                # enter dictionary for store the data\n",
    "                d2 = {}\n",
    "                try:\n",
    "                    d2[\"Content\"] = item.text\n",
    "                    time.sleep(2)\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                transcript_content.append(d2)\n",
    "\n",
    "            i += 1\n",
    "            print(i)\n",
    "         #-----------------------------------------------------------------------------------------------------#   \n",
    "        df = pd.concat([pd.DataFrame(transcript_title), pd.DataFrame(transcript_content)], axis=1)\n",
    "            \n",
    "        # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "        writer = pd.ExcelWriter('df_transcripts.xlsx', engine='xlsxwriter')\n",
    "            \n",
    "        # Convert the dataframe to an XlsxWriter Excel object.\n",
    "        df.to_excel(writer, sheet_name='Sheet1')\n",
    "            \n",
    "        # Close the Pandas Excel writer and output the Excel file.\n",
    "        writer.save()\n",
    "        \n",
    "Project3 = MSA8040_SeekingAlphaBot(\"patricks.s@hotmail.com\",\"msa80402020\")\n",
    "Project3.login()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3\n",
    "\n",
    "-- The last thing we need to do is to use techniques to parse the data from Phase 2 (content columns) and build the final dataset with the desired attributes.\n",
    "\n",
    "-- After Phase 2, we noticed that due to subscription requirements of the website, not every 2,000 transcripts have been fully scraped. In addition, this step will involve parsing the data in order to get a structured data by spliting the rows into columns and then understanding the logic to create each attribute separately. So, the size of the df_transcripts file has been reduced to 286 (df_transcripts_selection.xlsx) after further review and then reduced to 235 transcripts as a final dataset (final_dataset.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('df_transcripts_selection.xlsx')\n",
    "prow = pd.DataFrame(columns = ['Ticker','Company','Title','Date','Time','Speech','Participant name'])\n",
    "\n",
    "def run(context):\n",
    "    df_list=context.split(\"\\n\")\n",
    "    df_list =list(filter(lambda a: a != '', df_list ))\n",
    "    def listToString(s):  \n",
    "\n",
    "        # initialize an empty string \n",
    "        str1 = \" \" \n",
    "        # return string   \n",
    "        return (str1.join(s)) \n",
    "    def find_company(df):\n",
    "        Company=df_list[0].split('(')[0]\n",
    "        Ticker=df_list[0].split('(')[1].split(')')[0].split(':')[1]\n",
    "        a=df_list[0].split(')')[1].split(' ')[:-7]\n",
    "        Conference=listToString(a)\n",
    "        Month=df_list[0].split(')')[1].split(' ')[-7]\n",
    "        Date=df_list[0].split(')')[1].split(' ')[-6]\n",
    "        Year=df_list[0].split(')')[1].split(' ')[-5]\n",
    "        Time=df_list[0].split(')')[1].split(' ')[-3]\n",
    "        AM_PM=df_list[0].split(')')[1].split(' ')[-2]\n",
    "        Time_Zone=df_list[0].split(')')[1].split(' ')[-1]\n",
    "        df_Company= pd.DataFrame()\n",
    "        df_Company['Ticker']=[Ticker]\n",
    "        df_Company['Company']=[Company]\n",
    "        df_Company['Conference']=[Conference]\n",
    "        df_Company['Month']=[Month]\n",
    "        df_Company['Date']=[Date]\n",
    "        df_Company['Year']=[Year]\n",
    "        df_Company['Time']=[Time]\n",
    "        df_Company['AM_PM']=[AM_PM]\n",
    "        df_Company['Time_Zone']=[Time_Zone]\n",
    "        return df_Company\n",
    "    find_company=find_company(df)\n",
    "    def find_Company_Participants(df):\n",
    "        i=0\n",
    "        Company_Participants_list=[]\n",
    "        Conference_Call_Participants_list=[]\n",
    "        while i>=0:\n",
    "            i=i+1\n",
    "            if df_list[i]=='Conference Call Participants':\n",
    "                break\n",
    "            Company_Participants_list.append(i)   \n",
    "        while i==i:\n",
    "            i=i+1\n",
    "            if len(df_list[i])>100:\n",
    "                break\n",
    "            Conference_Call_Participants_list.append(i)\n",
    "        Company_Participants_list=Company_Participants_list[1:]\n",
    "        Conference_Call_Participants_list=Conference_Call_Participants_list[:-1]\n",
    "        Company_Participants_name=[]\n",
    "        Company_Participants_type=[]\n",
    "        for i in Company_Participants_list:\n",
    "            name_title=df_list[i]\n",
    "            Company_Participants_name.append((name_title.split('-')[0]))\n",
    "            Company_Participants_type.append(name_title.split('-')[1])\n",
    "        df_Company_Participants= pd.DataFrame( {'Participant name': Company_Participants_name, 'Company_Participants_type': Company_Participants_type})\n",
    "        return df_Company_Participants\n",
    "    find_Company_Participants=find_Company_Participants(df)\n",
    "    def find_Conference_Call_Participants(df):\n",
    "        i=0\n",
    "        Company_Participants_list=[]\n",
    "        Conference_Call_Participants_list=[]\n",
    "        while i>=0:\n",
    "            i=i+1\n",
    "            if df_list[i]=='Conference Call Participants':\n",
    "                break\n",
    "            Company_Participants_list.append(i)   \n",
    "        while i==i:\n",
    "            i=i+1\n",
    "            if len(df_list[i])>100:\n",
    "                break\n",
    "            Conference_Call_Participants_list.append(i)\n",
    "        Company_Participants_list=Company_Participants_list[1:]\n",
    "        Conference_Call_Participants_list=Conference_Call_Participants_list[:-1]\n",
    "        Conference_Call_Participants_name=[]\n",
    "        Conference_Call_Participants_type=[]\n",
    "        for i in Conference_Call_Participants_list:\n",
    "            name_title=df_list[i]\n",
    "            Conference_Call_Participants_name.append((name_title.split('-')[0]))\n",
    "            Conference_Call_Participants_type.append(name_title.split('-')[1])\n",
    "        df_Conference_Call_Participants= pd.DataFrame( {'Participant name': Conference_Call_Participants_name, 'Participant_Organization': Conference_Call_Participants_type})\n",
    "        return df_Conference_Call_Participants\n",
    "    find_Conference_Call_Participants=find_Conference_Call_Participants(df)\n",
    "\n",
    "    df_new=pd.DataFrame({'Speech': df_list,'Participant name':'','Section':'presentation'})\n",
    "    #####################name\n",
    "    name_index=[]\n",
    "    for i in find_Company_Participants['Participant name']:\n",
    "        name=(str(i)[:-1])\n",
    "        a=[i for i,x in enumerate(df_list) if x==name]\n",
    "        name_index=name_index+a\n",
    "    for i in find_Conference_Call_Participants['Participant name']:\n",
    "        name=(str(i)[:-1])\n",
    "        a=[i for i,x in enumerate(df_list) if x==name]\n",
    "        name_index=name_index+a\n",
    "    Operator=[i for i,x in enumerate(df_list) if x=='Operator']\n",
    "    name_index=name_index+Operator\n",
    "    name_index.sort()\n",
    "    for i in name_index:\n",
    "        df_new.iloc[i:,1]=df_new.iloc[i,0]\n",
    "   ####################QA\n",
    "    QA_index=[i for i,x in enumerate(df_list) if x=='Question-and-Answer Session']\n",
    "    for i in QA_index:\n",
    "        df_new.iloc[i:,2]='Question-and-Answer Session'\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    df_new=df_new[df_new['Speech'].str.len()>110]\n",
    "    df_new['Ticker']=find_company['Ticker'][0]\n",
    "    df_new['Company']=find_company['Company'][0]\n",
    "    \n",
    "    \n",
    "    clean_2=list(filter(lambda a: a != '',df_list[0].split(')')[1].split(' ')))\n",
    "    df_new['Title']=listToString(clean_2[:-7])\n",
    "    \n",
    "    \n",
    "    clean_1 =list(filter(lambda a: a != '', df_list[0].split(')')[1].split(' ') ))\n",
    "    df_new['Date']=listToString(clean_1[-6:-3])\n",
    "    df_new['Time']=listToString(df_list[0].split(')')[1].split(' ')[-3:])    \n",
    "    df_new['Participant name']=df_new['Participant name']+\" \"\n",
    "    \n",
    "    df_new=df_new[['Ticker','Company','Title','Date','Time','Section','Speech','Participant name']]\n",
    "    #####add Participant_Organization/Company_Participants_type\n",
    "    df_new=pd.merge(df_new,find_Company_Participants, how='left', left_on=['Participant name'], right_on=['Participant name'])\n",
    "    df_new=pd.merge(df_new,find_Conference_Call_Participants, how='left', left_on=['Participant name'], right_on=['Participant name'])\n",
    "    #prow=prow.append(df_new)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<len(df['Content']):\n",
    "    i=i+1\n",
    "    try:\n",
    "        prow =prow.append(run(df['Content'][i-1]))\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prow['Company_Participants_type']=prow['Company_Participants_type'].replace(np.nan, 'attendee')\n",
    "display(prow)\n",
    "\n",
    "prow.to_csv(\"final_dataset.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
